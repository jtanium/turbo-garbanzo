# turbo-garbanzo
Practicing multiple regression with Python using the dataset from https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho

* `Car details v3.csv` original data file from Kaggle
* `car_details_v3.csv` computable output file from `convert_car_details_v3.py`
* `convert_car_details_v3.py` reads `Car details v3.csv` and generates a new file that can be computed; it generates a boolean for each make/model combination
* `vehicle_prices.py` multiple regression script

Output:
```
Iteration 997000 => Loss: 455439883457.544372558593750
Iteration 998000 => Loss: 455439797039.625549316406250
Iteration 999000 => Loss: 455439710621.971679687500000

Weights: [[-1.44541140e-01 -3.42624718e-02 -4.69624517e-02  6.65999029e-03
   9.36925165e-02  1.23531047e-01  4.55351605e-01  1.90253705e-01
   4.50117341e-01  4.94466971e-03  3.74628875e-01  5.96533342e-01
   5.68964121e-01 -2.59808338e-02  1.86255717e-01  1.56023803e-02
   6.40166525e+00 -2.85604529e-02  8.96695170e-02  1.26046163e-01
  -7.37973361e-02 -2.10716642e-01 -9.19283869e-02 -2.46390696e-01
   1.70645643e-02 -1.83425185e-01 -1.69851251e-01 -1.57267088e-01
  -4.40967429e-01 -1.39180171e-02  2.15434475e-03 -2.22247507e-01
  -7.25050504e-02  1.03567385e-02 -6.15884704e-02 -3.14690522e-02
   0.00000000e+00 -1.84932459e-02 -4.59961177e-02 -4.63682193e-02
   2.18241382e-03 -4.88128911e-02  2.66735659e-02 -1.07087911e-01
  -2.51114571e-01 -7.88895163e-01 -3.59816857e-02 -2.22913675e-02
  -1.11653368e-01 -1.91247889e-01 -2.14574147e-01  7.55853513e-03
  -1.20113899e-01  3.34730908e-02 -3.54710699e-01 -1.01355379e-01
  -9.61406852e-02 -1.30207127e-02 -3.11443121e-03  6.11479290e-02
  -3.30153021e-01  5.66202255e-01 -7.29522519e-02  1.94130148e-02
   7.03629601e-02 -1.10099460e-01 -3.07856913e-01 -5.74769614e-01
   2.23375048e-02 -1.35731929e-02 -2.46085091e-01 -5.17999123e-02
   5.37663489e-03  4.18883199e-02 -6.55248474e-01  3.12738423e-02
   7.69807365e-03 -1.30529142e-02  2.53800847e-02  9.84757200e-01
   1.85920683e+00  3.73262220e-01  2.17897742e-01  5.54092340e-02
   3.21529776e-01  9.74775390e-03  2.01046786e-02  2.94166625e+00
  -3.97622522e+00  0.00000000e+00  0.00000000e+00 -1.01753686e-01
  -7.92272056e-03  1.07499219e-01  0.00000000e+00  3.38415028e-03
  -5.20887606e-02 -2.47749695e-02 -2.03810515e+00 -7.76502996e-02
   6.31580970e-03 -5.56677595e-01 -1.05552490e-02 -1.15328912e-01
   0.00000000e+00  7.65928022e-02 -6.58214574e-01 -1.14752615e+00
  -1.08124677e-01 -1.37175968e-02  1.64531922e-02  1.58298131e-01
  -8.61494087e-03  2.61630910e-01  1.12366131e-03 -1.89958455e-01
   7.27029443e-01 -8.29074269e-02  0.00000000e+00 -1.03799014e-02
  -7.33551083e-02  3.41431798e-02 -1.63686284e-01  1.20032701e-01
  -4.92960127e-03  5.81442400e-01 -1.56960584e-01  5.28464812e-01
  -1.71275274e-01  6.41649045e-03 -1.72753704e-01  4.00249757e-02
   6.17265793e-02  3.29469843e-01  2.41280553e-01  1.37633003e-01
   5.56995329e-02  9.51603759e-02  7.13807484e-02  3.95872206e-01
   6.61972121e-02 -7.36927532e-02  6.68710478e-03  4.19588565e-03
  -1.22850905e-01 -1.63177207e-01 -2.16153605e-02  9.13888219e-03
  -1.28202118e-02  0.00000000e+00  2.53585923e-02  4.10701052e-02
  -1.80963352e-02 -3.30105918e-03 -3.65934747e-02  3.45910663e-02
  -6.07479738e-02 -5.08133004e-03  2.19404104e-02  1.52222810e-02
   8.93126344e-02 -4.23258805e-02 -8.98304386e-02 -4.28383010e-01
   2.13459948e-02  3.90871641e-03 -1.17055283e-01 -2.82502188e-02
   0.00000000e+00  3.59645689e-02 -3.09311453e-02 -9.15102368e-01
  -9.05728234e-01 -1.88132597e-01 -5.28340309e-03 -7.77791600e-01
   5.65125620e-02 -8.85254567e-01 -3.91122445e-02 -7.97740822e-01
  -2.21421930e-01 -4.17383601e-02 -3.10287980e-02 -2.29999447e-02
  -3.28663814e-02 -6.10010021e-02  3.57402980e-01 -1.30363225e-01
  -2.68109844e-01  2.42240216e-01  1.04070225e-02 -1.83905013e+00
   3.92217162e-02  4.73252656e-02  0.00000000e+00 -4.95899623e-03
   1.37166943e-02 -1.00043747e-01 -1.62510851e-02 -3.19078602e-03
  -7.43775813e-02  0.00000000e+00 -2.41612788e-02 -1.34947578e-01
  -2.85674317e-01  5.05719585e-02  1.05821332e-01  7.92289068e-01
   2.00195150e+00  1.09753650e-01  2.21166721e-01 -5.61105010e-02
   2.75956456e+00  5.00089088e-03 -2.85299609e+00  1.57749213e+01
   2.08673692e+01 -2.10119103e+01  1.57749213e+01 -1.55204235e+01
  -3.99038916e-01  1.15347819e+01 -8.58420832e+00 -2.56406761e+00
  -5.31047088e-01 -1.26082140e+02 -4.73599169e+00  7.01811429e+01
   8.43671293e+02 -3.13037875e+01]]

A few predictions:
X[0] -> 111349.4713 (label: 450000)
X[1] -> 442855.3444 (label: 370000)
X[2] -> 348059.5545 (label: 158000)
X[3] -> 324304.3659 (label: 225000)
X[4] -> 274665.4641 (label: 130000)
```
Ran 1,000,000 iterations.